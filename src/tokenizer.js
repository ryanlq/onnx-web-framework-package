/**
 * Tokenizer åŠ è½½å™¨å’Œå·¥å…·ç±»
 *
 * æ”¯æŒä» URL åŠ è½½ tokenizer é…ç½®ï¼Œå¹¶æä¾›ç»Ÿä¸€çš„åˆ†è¯æ¥å£
 */

/**
 * Tokenizer åŸºç¡€æ¥å£
 * æ‰€æœ‰ tokenizer æ’ä»¶éƒ½éœ€è¦å®ç°è¿™ä¸ªæ¥å£
 */
export class ITokenizer {
  /**
   * ç¼–ç ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸º tokens
   * @param {string} text - è¾“å…¥æ–‡æœ¬
   * @returns {{ids: number[], attentionMask: number[], typeIds: number[]}}
   */
  encode(text) {
    throw new Error('encode() must be implemented by subclass');
  }

  /**
   * è§£ç ï¼šå°† tokens è½¬æ¢å›æ–‡æœ¬
   * @param {number[]} ids - token IDs
   * @returns {string}
   */
  decode(ids) {
    throw new Error('decode() must be implemented by subclass');
  }

  /**
   * è·å–è¯æ±‡è¡¨å¤§å°
   * @returns {number}
   */
  get vocabSize() {
    throw new Error('vocabSize getter must be implemented by subclass');
  }
}

/**
 * JSON Tokenizerï¼ˆHuggingFace æ ¼å¼ï¼‰
 * æ”¯æŒä» tokenizer.json åŠ è½½
 */
export class JSONTokenizer extends ITokenizer {
  constructor(config) {
    super();
    this.config = config;
    this.vocab = config.model?.vocab || {};
    this.merges = config.model?.merges || [];
    this.addedTokens = config.added_tokens || [];
    this._buildTrie();
  }

  /**
   * æ„å»º Trie æ ‘ç”¨äºå¿«é€ŸæŸ¥æ‰¾
   * @private
   */
  _buildTrie() {
    this.trie = {};
    for (const [token, id] of Object.entries(this.vocab)) {
      let node = this.trie;
      for (const char of token) {
        if (!node[char]) node[char] = {};
        node = node[char];
      }
      node._end = id;
    }
  }

  /**
   * ç¼–ç æ–‡æœ¬
   * @param {string} text
   * @returns {{ids: number[], attentionMask: number[], typeIds: number[]}}
   */
  encode(text) {
    // ç®€åŒ–çš„ BPE å®ç°ï¼ˆç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨ tokenizers.jsï¼‰
    const tokens = this._bpeEncode(text);
    const ids = tokens.map(t => this.vocab[t] || this.vocab['<unk>']);

    return {
      ids,
      attentionMask: ids.map(() => 1),
      typeIds: ids.map(() => 0)
    };
  }

  /**
   * BPE ç¼–ç ï¼ˆç®€åŒ–ç‰ˆï¼‰
   * @private
   */
  _bpeEncode(text) {
    // è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å®ç°
    // å®é™…ä½¿ç”¨æ—¶å»ºè®®é›†æˆ tokenizers.js æˆ– @nlpjs/bpe
    const words = text.split(/\s+/);
    const tokens = [];

    for (const word of words) {
      // ç®€å•çš„å­—ç¬¦çº§åˆ†è¯ä½œä¸º fallback
      if (this.vocab[word] !== undefined) {
        tokens.push(word);
      } else {
        // æŒ‰å­—ç¬¦åˆ‡åˆ†
        for (const char of word) {
          if (this.vocab[char] !== undefined) {
            tokens.push(char);
          }
        }
      }
    }

    return tokens;
  }

  /**
   * è§£ç  token IDs
   * @param {number[]} ids
   * @returns {string}
   */
  decode(ids) {
    const idToToken = Object.fromEntries(
      Object.entries(this.vocab).map(([k, v]) => [v, k])
    );
    return ids.map(id => idToToken[id] || '<unk>').join(' ');
  }

  get vocabSize() {
    return Object.keys(this.vocab).length;
  }
}

/**
 * Tokenizer åŠ è½½å™¨
 * ä» URL æˆ–æœ¬åœ°è·¯å¾„åŠ è½½ tokenizer é…ç½®
 */
export class TokenizerLoader {
  constructor() {
    this.cache = new Map();
  }

  /**
   * ä» URL åŠ è½½ tokenizer
   * @param {string} url - tokenizer.json æˆ– tokenizer.txt çš„ URL
   * @param {object} options - åŠ è½½é€‰é¡¹
   * @returns {Promise<ITokenizer>}
   */
  async loadFromUrl(url, options = {}) {
    const { useCache = true, format = 'auto' } = options;

    // æ£€æŸ¥ç¼“å­˜
    if (useCache && this.cache.has(url)) {
      return this.cache.get(url);
    }

    console.log(`ğŸ“¥ Loading tokenizer from: ${url}`);

    try {
      const response = await fetch(url);
      if (!response.ok) {
        throw new Error(`Failed to load tokenizer: ${response.statusText}`);
      }

      const text = await response.text();
      let tokenizer;

      // æ£€æµ‹æ ¼å¼
      const detectedFormat = format === 'auto' ? this._detectFormat(url, text) : format;

      switch (detectedFormat) {
        case 'json':
          const config = JSON.parse(text);
          tokenizer = new JSONTokenizer(config);
          break;

        case 'wordpiece':
          // WordPiece æ ¼å¼ (vocab.txt)
          const vocab = text.split('\n').filter(l => l.trim());
          tokenizer = this._createWordPieceTokenizer(vocab);
          break;

        default:
          throw new Error(`Unsupported tokenizer format: ${detectedFormat}`);
      }

      if (useCache) {
        this.cache.set(url, tokenizer);
      }

      console.log(`âœ… Tokenizer loaded successfully (vocab size: ${tokenizer.vocabSize})`);

      return tokenizer;
    } catch (error) {
      console.error(`âŒ Failed to load tokenizer:`, error);
      throw error;
    }
  }

  /**
   * ä»é…ç½®å¯¹è±¡åˆ›å»º tokenizer
   * @param {object} config - tokenizer é…ç½®
   * @param {string} type - tokenizer ç±»å‹
   * @returns {ITokenizer}
   */
  createFromConfig(config, type = 'json') {
    switch (type) {
      case 'json':
        return new JSONTokenizer(config);
      default:
        throw new Error(`Unsupported tokenizer type: ${type}`);
    }
  }

  /**
   * æ£€æµ‹ tokenizer æ ¼å¼
   * @private
   */
  _detectFormat(url, content) {
    if (url.endsWith('.json') || content.trim().startsWith('{')) {
      return 'json';
    }
    if (url.endsWith('.txt') || url.includes('vocab')) {
      return 'wordpiece';
    }
    return 'json'; // é»˜è®¤
  }

  /**
   * åˆ›å»º WordPiece tokenizer
   * @private
   */
  _createWordPieceTokenizer(vocab) {
    const vocabMap = {};
    vocab.forEach((token, idx) => {
      vocabMap[token] = idx;
    });

    return new JSONTokenizer({
      model: { vocab: vocabMap }
    });
  }

  /**
   * æ¸…é™¤ç¼“å­˜
   */
  clearCache() {
    this.cache.clear();
  }
}

/**
 * å•ä¾‹å®ä¾‹
 */
export const tokenizerLoader = new TokenizerLoader();

/**
 * ä¾¿æ·å‡½æ•°ï¼šä» URL åŠ è½½ tokenizer
 * @param {string} url
 * @param {object} options
 * @returns {Promise<ITokenizer>}
 */
export async function loadTokenizer(url, options) {
  return tokenizerLoader.loadFromUrl(url, options);
}

/**
 * ä¾¿æ·å‡½æ•°ï¼šä»é…ç½®åˆ›å»º tokenizer
 * @param {object} config
 * @param {string} type
 * @returns {ITokenizer}
 */
export function createTokenizer(config, type = 'json') {
  return tokenizerLoader.createFromConfig(config, type);
}
