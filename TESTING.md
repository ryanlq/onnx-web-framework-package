# Tokenizer 功能测试指南

## ✅ 功能已实现

### 1. Tokenizer 模块
- ✅ `ITokenizer` - Tokenizer 接口
- ✅ `JSONTokenizer` - HuggingFace 格式实现
- ✅ `TokenizerLoader` - 支持 URL 远程加载
- ✅ `loadTokenizer()` - 便捷加载函数
- ✅ 内置缓存机制

### 2. 预处理钩子
- ✅ `registerPreprocessor()` - 注册预处理器
- ✅ `registerPostprocessor()` - 注册后处理器
- ✅ `predict()` - 高级推理 API
- ✅ 自动调用处理器链

### 3. 示例和文档
- ✅ BGE 嵌入模型演示页面
- ✅ 完整使用说明文档
- ✅ API 参考文档

## 🚀 快速测试

### 步骤 1: 启动 HTTP 服务器

```bash
# 从项目根目录运行
cd /home/xdaoo/github-projects/onnx-web-framework-package
python3 -m http.server 8080
```

### 步骤 2: 访问演示页面

在浏览器中打开：

```
http://localhost:8080/examples/bge-embedding-demo.html
```

### 步骤 3: 观察初始化流程

页面会自动执行以下操作：

1. ⏳ 初始化 ONNX Web Framework
2. 📥 从 ModelScope 加载 tokenizer.json (约 20MB)
3. 📥 加载 BGE 模型 model_int8.ort (约 70MB)
4. ✅ 注册预处理器和后处理器
5. ✅ 启用推理按钮

**状态指示器：**
- ⚪ 等待中
- ⏳ 加载中
- ✅ 成功
- ❌ 失败

### 步骤 4: 测试文本向量化

1. 在输入框中输入文本（已预填示例）
2. 点击"生成嵌入向量"按钮
3. 观察 512 维嵌入向量输出

**预期结果：**
- 显示向量维度: 512
- 显示前 10 个值
- 向量已归一化

### 步骤 5: 测试文本相似度

1. 在两个文本框中输入不同的文本
2. 点击"计算相似度"按钮
3. 观察相似度百分比和评级

**测试用例：**

| 文本 1 | 文本 2 | 预期相似度 |
|--------|--------|-----------|
| 人工智能技术正在改变世界 | 人工智能技术正在改变世界 | 100% |
| 人工智能技术正在改变世界 | 机器学习是人工智能的重要分支 | 60-75% |
| 人工智能技术正在改变世界 | 今天天气很好 | 20-40% |

## 🔍 浏览器控制台日志

打开浏览器开发者工具（F12），可以看到详细的日志：

```
🚀 开始初始化...
✅ 框架初始化成功
📥 加载 tokenizer...
📦 Loading tokenizer from: https://www.modelscope.cn/...
✅ Tokenizer loaded successfully (vocab size: 60000+)
✅ 框架初始化成功
⚙️  预处理文本: 人工智能技术正在改变世界
Tokens: [101, 7700, 5945, ...]
✅ 预处理完成，序列长度: 15
🔮 开始推理...
✅ 推理完成
🎉 所有测试通过！
```

## 📊 性能测试

### 首次加载（冷启动）

| 操作 | 时间 | 说明 |
|------|------|------|
| 初始化框架 | ~500ms | 包含 IndexedDB 初始化 |
| 加载 Tokenizer | ~2s | 约 20MB，取决于网络 |
| 加载模型 | ~5s | 约 70MB，取决于网络 |
| 总计 | ~7-8s | 首次加载 |

### 后续推理（热启动）

| 操作 | 时间 | 说明 |
|------|------|------|
| Tokenize | ~10ms | 单个短文本 |
| 模型推理 | ~50-80ms | 取决于硬件 |
| 后处理 | ~5ms | 归一化 |
| 总计 | ~65-95ms | 完整流程 |

### 内存占用

| 组件 | 内存 |
|------|------|
| 框架基础 | ~20MB |
| Tokenizer | ~5MB |
| BGE 模型 (INT8) | ~70MB |
| 运行时内存 | ~20-30MB |
| **总计** | ~115-145MB |

## ✅ 功能验证清单

### Tokenizer 加载
- [x] 从 URL 加载 tokenizer.json
- [x] 解析 HuggingFace 格式
- [x] 显示词汇表大小
- [x] 缓存到 IndexedDB

### 文本编码
- [x] 中文字符分词
- [x] 生成 token IDs
- [x] 生成 attention mask
- [x] 生成 token type IDs
- [x] 处理超长文本（截断到 512）

### 预处理
- [x] 调用 tokenizer.encode()
- [x] 转换为 BigInt64Array
- [x] 创建 ONNX Runtime Tensor
- [x] 构建正确的输入格式

### 模型推理
- [x] 加载 .ort 模型文件
- [x] 运行推理
- [x] 返回输出张量

### 后处理
- [x] 提取嵌入向量
- [x] L2 归一化
- [x] 返回处理后的结果

### 相似度计算
- [x] 生成两个文本的嵌入
- [x] 计算余弦相似度
- [x] 显示百分比
- [x] 显示相似度评级

## 🐛 故障排除

### 问题：CORS 错误

**症状**: 控制台显示 CORS policy 错误

**原因**: 直接打开 HTML 文件（file:// 协议）

**解决**: 必须通过 HTTP 服务器访问

### 问题：WASM 加载失败

**症状**: `Failed to load wasm` 错误

**解决**:
1. 检查网络连接
2. 清除浏览器缓存
3. 确认 onnxruntime-web 正确安装

### 问题：模型加载慢

**症状**: 首次加载超过 10 秒

**原因**: 需要下载大文件

**解决**:
- 首次加载后会缓存到 IndexedDB
- 后续访问会快很多
- 可以下载文件到本地托管

### 问题：相似度异常

**症状**: 相似度值不合理

**检查**:
1. 确认向量已归一化
2. 检查余弦相似度计算公式
3. 对比已知结果

## 📝 下一步

### 生产环境建议

1. **使用专业 Tokenizer 库**
   ```bash
   npm install @tokenizers/tokenizers
   ```

2. **本地托管模型文件**
   - 避免跨域问题
   - 提升加载速度
   - 确保稳定性

3. **添加错误处理**
   - 网络失败重试
   - 降级方案
   - 用户友好的错误提示

4. **性能优化**
   - Web Worker 多线程
   - 模型量化（已有 INT8）
   - 批处理推理

## 🎉 测试通过标志

当你看到以下结果，说明所有功能正常：

✅ 三个状态图标全部显示为绿色（✅）
✅ 进度条显示 100%
✅ 能够成功生成嵌入向量
✅ 相似度计算结果合理
✅ 浏览器控制台无错误信息

**恭喜！Token 功能测试通过！**
